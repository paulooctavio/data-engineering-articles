{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from pyspark.sql import SparkSession\n",
    "from faker import Faker\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 19:48:20 WARN Utils: Your hostname, paulo resolves to a loopback address: 127.0.1.1; using 172.27.58.218 instead (on interface eth0)\n",
      "24/05/25 19:48:20 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/05/25 19:48:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"LazyEvaluation\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/05/25 19:56:00 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "24/05/25 19:56:00 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 84.47% for 8 writers\n",
      "24/05/25 19:56:02 WARN MemoryManager: Total allocation exceeds 95.00% (906,992,014 bytes) of heap memory\n",
      "Scaling row group sizes to 96.54% for 7 writers\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "fake = Faker()\n",
    "num_records = 1000\n",
    "activities = (\"running\", \"swimming\", \"reading\", \"cooking\", \"gaming\", \"reading\")\n",
    "for _ in range(num_records):\n",
    "    fake_data = pd.DataFrame(\n",
    "        {\n",
    "            \"user_id\": [fake.uuid4() for _ in range(num_records)],\n",
    "            \"activity\": [\n",
    "                fake.random_element(elements=activities) for _ in range(num_records)\n",
    "            ],\n",
    "            \"start_timestamp\": [\n",
    "                fake.date_time_this_year().timestamp() for _ in range(num_records)\n",
    "            ],\n",
    "            \"end_timestamp\": [\n",
    "                fake.date_time_this_year().timestamp() + random.randint(0, 86400)\n",
    "                for _ in range(num_records)\n",
    "            ],\n",
    "            \"location\": [fake.city() for _ in range(num_records)],\n",
    "            \"age\": [fake.random_int(min=18, max=80) for _ in range(num_records)],\n",
    "        }\n",
    "    )\n",
    "\n",
    "# Convert Pandas DataFrame to PySpark DataFrame\n",
    "user_activity_df = spark.createDataFrame(fake_data)\n",
    "\n",
    "user_activity_df.write.parquet(\"tmp/data/user_activity\", mode=\"overwrite\")\n",
    "user_activity_df = spark.read.parquet(\"tmp/data/user_activity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+-------------------+-------------------+-------------------+---+\n",
      "|             user_id|activity|    start_timestamp|      end_timestamp|           location|age|\n",
      "+--------------------+--------+-------------------+-------------------+-------------------+---+\n",
      "|24b142ce-0f7e-449...| reading|1.707829383915254E9|1.712826204423252E9|   West Ronaldmouth| 38|\n",
      "|52e12b9c-b910-491...| reading|1.707039871804524E9|1.710571944809592E9|  South Richardbury| 65|\n",
      "|fb52fdb6-0dc6-420...|swimming|1.704138366546621E9| 1.70540251114133E9|West Stephenchester| 26|\n",
      "|1d83172a-a3b9-41d...|  gaming|1.708359939943264E9|1.711729919060253E9|     West Nathaniel| 48|\n",
      "|3e5ce184-52cd-4e5...|swimming|1.706404599417025E9| 1.71497861549062E9|  East Jeffreyville| 42|\n",
      "+--------------------+--------+-------------------+-------------------+-------------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the user activity data\n",
    "user_activity_df = spark.read.parquet(\"tmp/data/user_activity\")\n",
    "user_activity_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "df_calculated = user_activity_df.withColumn(\n",
    "    \"activity_duration\",\n",
    "    user_activity_df[\"end_timestamp\"] - user_activity_df[\"start_timestamp\"],\n",
    ")\n",
    "df_filtered = df_calculated.filter(df_calculated[\"age\"] > 21)\n",
    "df_selected = df_filtered.select(\"user_id\", \"activity_duration\")\n",
    "df_result = df_selected.groupBy(\"user_id\").sum(\"activity_duration\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- HashAggregate(keys=[user_id#375], functions=[sum(activity_duration#413)])\n",
      "   +- Exchange hashpartitioning(user_id#375, 200), ENSURE_REQUIREMENTS, [plan_id=222]\n",
      "      +- HashAggregate(keys=[user_id#375], functions=[partial_sum(activity_duration#413)])\n",
      "         +- Project [user_id#375, (end_timestamp#378 - start_timestamp#377) AS activity_duration#413]\n",
      "            +- Filter (isnotnull(age#380L) AND (age#380L > 21))\n",
      "               +- FileScan parquet [user_id#375,start_timestamp#377,end_timestamp#378,age#380L] Batched: true, DataFilters: [isnotnull(age#380L), (age#380L > 21)], Format: Parquet, Location: InMemoryFileIndex(1 paths)[file:/mnt/c/Users/paulo_ufva11y/OneDrive/Documentos/Estudos/data-engin..., PartitionFilters: [], PushedFilters: [IsNotNull(age), GreaterThan(age,21)], ReadSchema: struct<user_id:string,start_timestamp:double,end_timestamp:double,age:bigint>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_result.explain()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
