{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import broadcast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = (\n",
    "    SparkSession.builder.appName(\"Broadcast Example\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "# Surprass warnings\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# Disable Spark auto broadcast\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", -1)\n",
    "\n",
    "# Disable Sort Merge Join\n",
    "spark.conf.set(\"spark.sql.join.preferSortMergeJoin\", \"false\")\n",
    "\n",
    "# Disabling WholeStageCodegen to see the actual physical operators in the execution plan\n",
    "spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1_000_000\n",
    "fact_table_data = [\n",
    "    (i % 1000, round(random.uniform(10.0, 1000.0), 2)) for i in range(1, size)\n",
    "]\n",
    "fact_table_df = spark.createDataFrame(fact_table_data, [\"id\", \"value\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = 1_000\n",
    "dimension_table_data = [\n",
    "    (i, random.choice([\"A\", \"B\", \"C\", \"D\", \"E\", \"F\"])) for i in range(1, size)\n",
    "]\n",
    "dimension_table_df = spark.createDataFrame(dimension_table_data, [\"id\", \"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df_without_broadcast = fact_table_df.hint(\"SHUFFLE_HASH\").join(\n",
    "    dimension_table_df, on=\"id\", how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df_with_broadcast = fact_table_df.join(\n",
    "    broadcast(dimension_table_df), on=\"id\", how=\"inner\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:==>                (1 + 7) / 8][Stage 1:>                  (0 + 1) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExecution time: 3.10 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "joined_df_without_broadcast.count()\n",
    "execution_time = time.time() - start_time\n",
    "print(f\"\\tExecution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tExecution time: 0.59 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "joined_df_with_broadcast.count()\n",
    "execution_time = time.time() - start_time\n",
    "print(f\"\\tExecution time: {execution_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#137L, value#138, category#142]\n",
      "   +- BroadcastHashJoin [id#137L], [id#141L], Inner, BuildRight, false\n",
      "      :- Filter isnotnull(id#137L)\n",
      "      :  +- Scan ExistingRDD[id#137L,value#138]\n",
      "      +- BroadcastExchange HashedRelationBroadcastMode(List(input[0, bigint, false]),false), [plan_id=1285]\n",
      "         +- Filter isnotnull(id#141L)\n",
      "            +- Scan ExistingRDD[id#141L,category#142]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df_with_broadcast.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Project [id#137L, value#138, category#142]\n",
      "   +- ShuffledHashJoin [id#137L], [id#141L], Inner, BuildLeft\n",
      "      :- Exchange hashpartitioning(id#137L, 200), ENSURE_REQUIREMENTS, [plan_id=1313]\n",
      "      :  +- Filter isnotnull(id#137L)\n",
      "      :     +- Scan ExistingRDD[id#137L,value#138]\n",
      "      +- Exchange hashpartitioning(id#141L, 200), ENSURE_REQUIREMENTS, [plan_id=1314]\n",
      "         +- Filter isnotnull(id#141L)\n",
      "            +- Scan ExistingRDD[id#141L,category#142]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "joined_df_without_broadcast.explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
